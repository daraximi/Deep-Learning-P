{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Ali-Alameer/Deep-Learning/blob/main/week4_convolutional_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkfgjYcE9R1z"
   },
   "source": [
    "Convolutional Layers: These layers perform convolution operations on the input images to extract relevant features. The Conv2D layers use small square filters and a rectified linear unit (relu) activation function to learn and detect different patterns in the images.\n",
    "\n",
    "Pooling Layer: The MaxPooling2D layer downsamples the feature maps obtained from the convolutional layers by selecting the maximum value within each pooling region. This helps reduce the spatial dimensions while retaining the most important features.\n",
    "\n",
    "Flatten Layer: This layer transforms the multidimensional feature maps into a flat vector. It allows the subsequent fully connected layers to process the extracted features.\n",
    "\n",
    "Dense Layers: These fully connected layers use the extracted features to classify the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iFM-1YGS87Xe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling MaxPooling2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 2 from 1 for '{{node sequential_2_1/max_pooling2d_3_1/MaxPool2d}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](sequential_2_1/batch_normalization_3_1/batchnorm/add_1)' with input shapes: [?,1,1,64].\u001b[0m\n\nArguments received by MaxPooling2D.call():\n  • inputs=tf.Tensor(shape=(None, 1, 1, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     47\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/sem2/trienv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/sem2/trienv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling MaxPooling2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 2 from 1 for '{{node sequential_2_1/max_pooling2d_3_1/MaxPool2d}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](sequential_2_1/batch_normalization_3_1/batchnorm/add_1)' with input shapes: [?,1,1,64].\u001b[0m\n\nArguments received by MaxPooling2D.call():\n  • inputs=tf.Tensor(shape=(None, 1, 1, 64), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "X_train = X_train.reshape(-1, 8, 8, 1) / 16.0  # Normalize the pixel values to the range [0, 1] and reshape for CNN\n",
    "X_val = X_val.reshape(-1, 8, 8, 1) / 16.0\n",
    "X_test = X_test.reshape(-1, 8, 8, 1) / 16.0\n",
    "\n",
    "# Convert the target labels to categorical format\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Create a convolutional neural network (CNN)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(8, 8, 1)))\n",
    "model.add(BatchNormalization())  # Batch normalization after Conv layer\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Second convolutional layer with batch normalization\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())  # Batch normalization after Conv layer\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9AaUD4J-3mP"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = y_pred.argmax(axis=1)\n",
    "y_test_labels = y_test.argmax(axis=1)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average='weighted')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test_labels, y_pred_labels, average='weighted')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test_labels, y_pred_labels, average='weighted')\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sP1mNT5U-xTR"
   },
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.arange(1, len(history.history['accuracy']) + 1), history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(np.arange(1, len(history.history['accuracy']) + 1), history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTeA9ky6BOm-"
   },
   "source": [
    "Let's have a look at our input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtF1PW7V9nyQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Get the number of samples and image dimensions\n",
    "num_samples, img_height, img_width = X_train.shape[0], digits.images[0].shape[0], digits.images[0].shape[1]\n",
    "\n",
    "# Visualize the first few training images\n",
    "num_images_to_visualize = 5\n",
    "fig, axes = plt.subplots(1, num_images_to_visualize, figsize=(12, 4))\n",
    "\n",
    "for i in range(num_images_to_visualize):\n",
    "    axes[i].imshow(digits.images[i], cmap='gray')\n",
    "\n",
    "# Display image size and shape information\n",
    "print(\"Number of training samples:\", num_samples)\n",
    "print(\"Image size: {} x {}\".format(img_height, img_width))\n",
    "print(\"Image shape:\", digits.images[0].shape)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNjCcrVYCiOr"
   },
   "source": [
    "# Visualize the label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOgNkidDBh-B"
   },
   "outputs": [],
   "source": [
    "label_counts = np.sum(y_train, axis=0)\n",
    "\n",
    "# Visualize the label distribution\n",
    "labels = range(10)\n",
    "plt.bar(labels, label_counts)\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.xticks(labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNuAbRVRVBdOLaugLZZhGUF",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "trienv",
   "language": "python",
   "name": "trienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
